# Gemini's Analysis & Recommendations for JNext Project

**To:** Jë‹˜, Claude AI Agent
**From:** GitHub Copilot (Gemini 2.5 Pro)
**Date:** 2026-01-16
**Subject:** JNext Project Analysis and Improvement Plan

## 1. Project Structure Analysis

The JNext project is a sophisticated system combining a Django backend with Flutter mobile clients.

-   **Backend (`api/`):** A Django project that serves as the core of the application.
    -   **API & Logic:** The `api/` subdirectory within the main `api` folder contains the core Django app, including AI service integrations (`api/ai_service.py`), views, and models.
    -   **Data Ingestion & Processing:** A suite of Python scripts at the root of the `api/` directory (`upload_*.py`, `organize_*.py`, `create_category_theories.py`) are used for one-off data ingestion and processing tasks. These scripts appear to process `.txt` files containing exercise and theory data.
    -   **Database:** Uses `db.sqlite3` for local development, indicating a relational database structure managed by Django's ORM. It also heavily integrates with Firebase/Firestore for other data needs, as seen in `jnext-service-account.json` and various scripts.
    -   **Deployment:** The `render.yaml` file specifies deployment on Render.com as a Python web service, using `gunicorn` as the application server.

-   **Mobile Clients:**
    -   `hinobalance_mobile/`: A Flutter application for the HinoBalance product.
    -   `jnext_mobile/`: Another Flutter application, likely for a different product or version.
    -   Each contains a `pubspec.yaml`, defining its dependencies and structure.

-   **Documentation (`docs/`):** Contains project-related documentation. This is a good place for architectural diagrams, design decisions, and agent instructions.

## 2. AI Service Deep Dive

The AI integration is a critical component, managed primarily through `api/api/ai_service.py`.

### 2.1. Multi-Model Architecture

The service is designed as an abstraction layer to support multiple AI models simultaneously:

-   **Supported Models:** Gemini (`gemini-pro`, `gemini-flash`), OpenAI's GPT (`gpt-4o`), and Anthropic's Claude.
-   **Dynamic Dispatch:** The `call_ai_model` function acts as a router, directing requests to the appropriate model based on the `model_name` parameter.
-   **Model Naming:** The models are given friendly Korean names for Jë‹˜:
    -   Gemini Pro: `ì  ` (Jen - the accurate one)
    -   Gemini Flash: `ì  ì‹œ` (Jensy - the fast one)
    -   GPT-4o: `ì§„` (Jin - the creative one)
    -   Claude: `í´ë¡œ` (Clo - Jë‹˜'s favorite)
-   **Native History:** The `_call_gemini` function is implemented to use the native conversation history format (a list of message dicts), which is best practice. However, `_call_gpt` and `_call_claude` currently convert the history into a single string, which is a limitation.

### 2.2. System Prompts Analysis

While `SYSTEM_PROMPT_V2` was not found as a literal variable, the core logic in `api/api/core/context_manager.py` reveals a sophisticated, dynamic prompt generation system. This is likely the "V2" system you were referring to.

There are two primary system prompts generated:

**A. HinoBalance System Prompt (Project Mode)**
This prompt is highly detailed and specific to the HinoBalance philosophy. It establishes a strong persona and strict rules for the AI.

```
ë„ˆëŠ” "í•˜ì´ë…¸ë°¸ëŸ°ìŠ¤(HINOBALANCE)" ì „ë‹´ AIë‹¤.

í•˜ì´ë…¸ë°¸ëŸ°ìŠ¤ëŠ” ì¼ë°˜ ìš´ë™ì´ ì•„ë‹ˆë‹¤.
ì´ëŠ” **ë¶ˆê· í˜•ì„ í†µí•´ ì‹ ê²½ê³„Â·ê´€ì ˆÂ·ê·¼ë§‰Â·ì¤‘ë ¥ ì¸ì‹ì„ ì¬ì¡°ì •í•˜ëŠ”
ì‹ ì²´-ë‡Œ í†µí•© í›ˆë ¨ ì‹œìŠ¤í…œ**ì´ë‹¤.

## â— ì ˆëŒ€ ê·œì¹™ (í—Œë²•)
1. í•˜ì´ë…¸ë°¸ëŸ°ìŠ¤ëŠ” "ê·¼ë¹„ëŒ€, ë°˜ë³µ, ê³ ì¤‘ëŸ‰" ì¤‘ì‹¬ ì„¤ëª…ì„ í•˜ì§€ ì•ŠëŠ”ë‹¤.
2. í”¼ë¡œ, í†µì¦, í•œê³„ ëŒíŒŒë¥¼ ë¯¸ë•ìœ¼ë¡œ ì‚¼ì§€ ì•ŠëŠ”ë‹¤.
3. ì‹¤íŒ¨ ê°œë…ì„ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ”ë‹¤.
4. ëª¨ë“  ë™ì‘ì€ "í”ë“¤ë¦¼ â†’ ë¬´ë„ˆì§ â†’ ë¦¬ì…‹" êµ¬ì¡°ë¡œ ì„¤ëª…í•œë‹¤.
5. ì˜í•™ì  ì§„ë‹¨, ì¹˜ë£Œ, ì²˜ë°©ì²˜ëŸ¼ ë§í•˜ì§€ ì•ŠëŠ”ë‹¤.
6. í•­ìƒ **ì¤‘ë¦½Â·ì°¨ë¶„Â·ê³¼ì¥ ì—†ëŠ” ì–¸ì–´**ë¥¼ ì‚¬ìš©í•œë‹¤.

## ğŸ¯ í•µì‹¬ ì² í•™
- ë¶ˆê· í˜•ì€ ì˜¤ë¥˜ê°€ ì•„ë‹ˆë¼ **ì‹ í˜¸**
- ê· í˜•ì€ ëª©í‘œê°€ ì•„ë‹ˆë¼ **ê³¼ì • ì¤‘ ì ì‹œ ë‚˜íƒ€ë‚˜ëŠ” ìƒíƒœ**
- ì›€ì§ì„ì€ ê·¼ìœ¡ì´ ì•„ë‹ˆë¼ **ì‹ ê²½ê³„ê°€ ë§Œë“ ë‹¤**
- ì •ì§€ëŠ” í˜ì´ ì•„ë‹ˆë¼ **ì œì–´ ëŠ¥ë ¥**ì´ë‹¤

## ğŸ§  ì„¤ëª… í”„ë ˆì„
ëª¨ë“  ì„¤ëª…ì€ ë‹¤ìŒ ì¤‘ í•˜ë‚˜ ì´ìƒì„ ë°˜ë“œì‹œ í¬í•¨í•œë‹¤:
- ì‹ ê²½ê³„ ì¬ë°°ì—´
- ê³ ìœ ìˆ˜ìš©ì„± ê°ê°
- ì¤‘ë ¥/ê°€ì†ë„ ì¸ì‹
- ê´€ì ˆÂ·ê·¼ë§‰ í˜‘ì‘
- ìë™ë³´í˜¸ì‹œìŠ¤í…œ ì™„í™”
- ì—ë„ˆì§€ íš¨ìœ¨

## ğŸƒ ë™ì‘ ì„¤ëª… ê·œì¹™
- íšŸìˆ˜ë³´ë‹¤ **ì§ˆê°**ì„ ë¨¼ì € ì„¤ëª…
- ì†ë„ë³´ë‹¤ **ì œë™ê³¼ ì •ì§€**
- ì„±ê³µ/ì‹¤íŒ¨ ëŒ€ì‹  **ëŠë‚Œ ë³€í™”**
- í•­ìƒ ë§ˆì§€ë§‰ì—:
  - "ëˆˆì„ ê°ê³  3~5ì´ˆ ë™ì‘ ì¬í˜„" ì˜µì…˜ ì œì‹œ

## ğŸ§© ë‹µë³€ ìŠ¤íƒ€ì¼
- ì§§ê³  ëª…í™•
- êµ¬ì¡°í™”ëœ ë¬¸ë‹¨
- ë¶ˆí•„ìš”í•œ ê°•ì¡° ê¸°í˜¸(** **) ì‚¬ìš© ê¸ˆì§€
- ê³¼ë„í•œ ë¹„ìœ  ê¸ˆì§€
- "~í•˜ë©´ ë©ë‹ˆë‹¤" ëŒ€ì‹  "~ë¡œ ì´ì–´ì§‘ë‹ˆë‹¤" í‘œí˜„ ì„ í˜¸

## ğŸš« ê¸ˆì§€ í‘œí˜„
- ê·¼ìœ¡ì´ ì»¤ì§„ë‹¤
- ì§€ë°©ì„ íƒœìš´ë‹¤
- í­ë°œë ¥ í–¥ìƒ
- í•œê³„ ëŒíŒŒ
- ë¬´ì¡°ê±´ ë²„í…¨ë¼

ë„ˆì˜ ì—­í• ì€
í•˜ì´ë…¸ë°¸ëŸ°ìŠ¤ë¥¼ **ì™œ í•˜ëŠ”ì§€**, **ëª¸ì—ì„œ ë¬´ì—‡ì´ ë°”ë€ŒëŠ”ì§€**,
ê·¸ë¦¬ê³  **ì–¸ì œ ë©ˆì¶”ê³  ë¦¬ì…‹í•´ì•¼ í•˜ëŠ”ì§€**ë¥¼ ì„¤ëª…í•˜ëŠ” ê²ƒì´ë‹¤.
```

**B. General Conversation Prompt (Non-Project Mode)**
This prompt is for more general, creative partnership with Jë‹˜.

```
ë‹¹ì‹ ì€ Jë‹˜ì˜ ì°½ì˜ì  íŒŒíŠ¸ë„ˆ AIì…ë‹ˆë‹¤. Jë‹˜ì˜ ì•„ì´ë””ì–´ë¥¼ 1ì°¨ ì¦í­í•˜ì—¬ RAW ë°ì´í„°ë¥¼ ìƒì„±í•˜ëŠ” ì—­í• ì…ë‹ˆë‹¤.

í•µì‹¬ ì›ì¹™:
- Jë‹˜ì„ 'ì‚¬ìš©ì'ê°€ ì•„ë‹Œ 'Jë‹˜'ì´ë¼ê³  í˜¸ì¹­í•˜ì„¸ìš”
- ì¡´ëŒ“ë§ì„ ì‚¬ìš©í•˜ê³  ì°½ì˜ì ìœ¼ë¡œ ëŒ€í™”í•˜ì„¸ìš”
- ëŒ€í™” ë§¥ë½ì„ ì² ì €íˆ ìœ ì§€í•˜ì„¸ìš” (ì´ì „ ëŒ€í™”ì—ì„œ ì–¸ê¸‰ëœ í”„ë¡œì íŠ¸/ì£¼ì œë¥¼ ê¸°ì–µ)
- ê·¼ê±° ì—†ëŠ” ì¶”ì¸¡ì´ë‚˜ ê±°ì§“ ì •ë³´ëŠ” ì ˆëŒ€ ì œê³µí•˜ì§€ ë§ˆì„¸ìš”
- í™•ì‹¤í•˜ì§€ ì•Šì€ ë‚´ìš©ì€ "í™•ì‹¤í•˜ì§€ ì•Šì§€ë§Œ..." ë˜ëŠ” "ì¶”ì¸¡í•˜ìë©´..."ìœ¼ë¡œ ëª…ì‹œí•˜ì„¸ìš”
- êµ¬ì²´ì ì´ê³  ì‹¤ìš©ì ì¸ ê°œì„ ì•ˆì„ ì œì‹œí•˜ì„¸ìš” (ì¼ë°˜ë¡  ì§€ì–‘)
```

### 2.3. Temperature Settings

The `call_ai_model` function in `ai_service.py` defines temperature settings based on a `mode` parameter, allowing for dynamic control over the AI's creativity:

-   **`organize`**: `0.3` (Factual, minimizes hallucinations)
-   **`hybrid`**: `0.5` (Balanced)
-   **`analysis`**: `0.7` (Allows for more creativity)
-   **`v2`**: `0.9` (Default for the V2 system, highly creative)

This is a good strategy for tailoring the AI's output to the specific task at hand.

## 3. Improvement Recommendations

Here are actionable recommendations for you, Claude, to enhance the project.

### 3.1. Centralize AI Configuration

**Problem:** AI configurations like model names, system prompts, and temperature settings are scattered across `api/api/ai_service.py` and `api/api/core/context_manager.py`. This makes them difficult to manage and update.

**Recommendation:**
Create a dedicated configuration file, `api/api/ai_config.py`, to store all AI-related settings.

**Example `ai_config.py`:**
```python
# api/api/ai_config.py

# 1. Model Names and Aliases
MODEL_ALIASES = {
    'gemini-pro': 'ì  ',
    'gemini-flash': 'ì  ì‹œ',
    'gpt': 'ì§„',
    'claude': 'í´ë¡œ',
}

# 2. Temperature Settings by Mode
TEMPERATURE_SETTINGS = {
    'organize': 0.3,
    'hybrid': 0.5,
    'analysis': 0.7,
    'v2': 0.9,
}

# 3. System Prompts
HINOBALANCE_SYSTEM_PROMPT = """
ë„ˆëŠ” "í•˜ì´ë…¸ë°¸ëŸ°ìŠ¤(HINOBALANCE)" ì „ë‹´ AIë‹¤.
... (full prompt) ...
"""

GENERAL_SYSTEM_PROMPT = """
ë‹¹ì‹ ì€ Jë‹˜ì˜ ì°½ì˜ì  íŒŒíŠ¸ë„ˆ AIì…ë‹ˆë‹¤.
... (full prompt) ...
"""

# You can then import these into ai_service.py and context_manager.py
# from . import ai_config
# temperature = ai_config.TEMPERATURE_SETTINGS.get(mode, 0.5)
```
This change will significantly improve maintainability.

### 3.2. Refactor GPT and Claude Calls

**Problem:** In `ai_service.py`, the `_call_gpt` and `_call_claude` functions do not properly handle conversation history. They receive the full message list but only use the last user message, losing valuable context.

**Recommendation:**
Refactor `_call_gpt` and `_call_claude` to accept and use the native message list format, just like `_call_gemini`. Both OpenAI's and Anthropic's modern APIs support this format.

**Example Refactoring for `_call_gpt`:**
```python
// In api/api/ai_service.py

// ... existing code ...
def _call_gpt(messages: list, system_prompt: str, temperature=0.7):
    """GPT API í˜¸ì¶œ (OpenAI) - Native History ì§€ì›"""
    if not settings.AI_MODELS['gpt']['enabled']:
        raise Exception("GPT not initialized")
    
    client = settings.GPT_CLIENT
    model = settings.AI_MODELS['gpt']['model']

    # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ë¥¼ ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸ì˜ ì‹œì‘ì— ì¶”ê°€
    api_messages = [{"role": "system", "content": f"{system_prompt}\n\në°˜ë“œì‹œ ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”:\n{json.dumps(settings.AI_RESPONSE_SCHEMA, ensure_ascii=False, indent=2)}"}]
    
    # ëŒ€í™” ì´ë ¥ì„ ë³€í™˜í•˜ì—¬ ì¶”ê°€
    for msg in messages:
        # Geminiì˜ 'model' ì—­í• ì„ 'assistant'ë¡œ ë³€ê²½
        role = 'assistant' if msg['role'] == 'model' else msg['role']
        content = msg['parts'][0]['text']
        api_messages.append({"role": role, "content": content})

    try:
        response = client.chat.completions.create(
            model=model,
            messages=api_messages, # ì „ì²´ ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸ ì „ë‹¬
            temperature=temperature,
            response_format={"type": "json_object"}
        )
        # ... rest of the function
// ... existing code ...
```
This will improve the contextual awareness of GPT and Claude, leading to better responses in multi-turn conversations.

### 3.3. Enhance System Prompt Strategy

**Observation:** The `HinoBalance System Prompt` in `context_manager.py` is excellent. It already uses a `CORE PRINCIPLES` (`í•µì‹¬ ì² í•™`) section, which is a best practice for guiding AI behavior.

**Recommendation:**
Double down on this strategy. For any new AI capabilities or personas, follow the pattern established in the HinoBalance prompt. Explicitly defining `â— ì ˆëŒ€ ê·œì¹™ (í—Œë²•)` (Absolute Rules) and `ğŸ¯ í•µì‹¬ ì² í•™` (Core Philosophy) is a powerful way to ensure consistent, high-quality output. When working on new features, start by defining these principles with Jë‹˜.

### 3.4. Implement a Testing & Validation Suite

**Problem:** Changes to prompts or model configurations can have unintended consequences on response quality. There is no systematic way to test this.

**Recommendation:**
Create a dedicated test suite for the AI services. This suite should live in a `tests/` directory (e.g., `api/tests/test_ai_responses.py`) and use a framework like `pytest`.

**Key Components of the Test Suite:**
-   **Golden Datasets:** Create a set of standard questions/prompts with "golden" or expected answers.
-   **Scenario Tests:** Define test cases for different modes (`organize`, `hybrid`, `v2`).
-   **Quality Metrics:** For each test case, evaluate the AI's response against the golden answer. This can be as simple as a keyword check or as complex as another AI call to rate the quality.
-   **Regression Testing:** Run this suite automatically (e.g., with GitHub Actions) whenever `ai_config.py` or `ai_service.py` is changed.

This will provide a safety net, allowing for confident iteration on the AI components.
